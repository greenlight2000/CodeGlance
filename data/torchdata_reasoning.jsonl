{"ID":"0","code":"from torchdata.datapipes.iter import *\nfrom torchdata.datapipes.utils import *\nfrom torchdata.datapipes.map import *\ndef generate_ans(file_list, train_batch_size=64):\n    url_dp = IterableWrapper(file_list)\n    data_dp = FileOpener(url_dp, mode=\"b\")\n    data= data_dp.parse_csv()\n    data = data.shuffle().batch(batch_size=train_batch_size, drop_last=False)\n    return data\n","test":"file_list = ['<real_path>\/test_data.csv']#this csv file has 1000 lines\ntrain_batch_size = 64\nassert len(list(generate_ans(file_list, train_batch_size))) == <ground_truth> ","result":"16","code_lines":6,"cyclomatic_complexity":1}
{"ID":"1","code":"from torchdata.datapipes.iter import *\ndef decode(item):\n    key, value = item\n    if key.endswith(\".txt\"):\n        return key, value.read().decode(\"utf-8\")\n    if key.endswith(\".bin\"):\n        return key, value.read().decode(\"utf-8\")\n    \ndef generate_ans(file_list):\n    dp = FileOpener(file_list, mode=\"b\").load_from_tar().map(decode).webdataset()\n    return dp\n","test":"file_list = ['<real_path>\/test_data.tar']#the tar file can be depressed as a .bin file and a txt file\nassert len(list(generate_ans(file_list))) == <ground_truth>","result":"1","code_lines":9,"cyclomatic_complexity":4}
{"ID":"10","code":"from torchdata.datapipes.iter import *\ndef generate_ans(csv_files, buffer_size):\n    train_dp = FileOpener(csv_files)\n    train_dp = train_dp.parse_csv()\n    train_dp = train_dp.shuffle(buffer_size=buffer_size)\n    train_dp = train_dp.sharding_filter()\n    return train_dp\n","test":"csv_files = ['<real_path>\/test_data.csv']#this csv file has 1000 lines\nbuffer_size = 1\nassert len(list(generate_ans(csv_files, buffer_size))) == <ground_truth>","result":"1000","code_lines":6,"cyclomatic_complexity":1}
{"ID":"102","code":"from torchdata.datapipes.iter import IterableWrapper, FileLister, FileOpener, StreamReader\ndef generate_ans(file_dir):\n    datapipe = FileLister(file_dir).filter(lambda fname: fname.endswith('.csv'))\n    datapipe = IterableWrapper(datapipe)\n    datapipe = FileOpener(datapipe, mode='t')\n    summary_dp = datapipe.parse_csv(skip_lines=0)\n    dataset = summary_dp\n    return dataset\n","test":"file_dir = '<real_path>\/'#has 1 csv file and the csv file has 1000 lines\nassert len(list(generate_ans(file_dir))) == <ground_truth>","result":"1000","code_lines":7,"cyclomatic_complexity":1}
{"ID":"103","code":"from torchdata.datapipes.iter import *\ndef filter_fn(data):\n    return data % 2 == 0\ndef map_fn(data):\n    return data + 1\ndef generate_ans(data_list, filter_fn=filter_fn, map_fn=map_fn):\n    dp = IterableWrapper(data_list)\n    dp = dp.filter(filter_fn=filter_fn)\n    dp = dp.map(map_fn)\n    dp1,dp2 = dp.fork(2)\n    dp = dp1.zip(dp2)\n    return dp\n","test":"data_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nassert len(list(generate_ans(data_list, filter_fn, map_fn))) == <ground_truth>","result":"5","code_lines":11,"cyclomatic_complexity":3}
{"ID":"109","code":"from torchdata.datapipes.iter import *\ndef generate_ans(dp1,dp2, batch_size=2):\n    dp_zip = dp1.zip(dp2) \n    dp_zip = dp_zip.shuffle()\n    dp_zip = dp_zip.batch(batch_size=batch_size)\n    return dp_zip\n","test":"dp1 = IterableWrapper([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ndp2 = IterableWrapper([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\nbatch_size = 2\nassert len(list(generate_ans(dp1,dp2))) == <ground_truth>","result":"5","code_lines":5,"cyclomatic_complexity":1}
{"ID":"11","code":"from torchdata.datapipes.iter import *  \ndef filter_fn(filename):\n    return filename.endswith(\".csv\")\n  \ndef generate_ans(data_dir: str = \".\"):  \n    dp = FileLister(data_dir)  \n    dp = Filter(dp, filter_fn=filter_fn)  \n    dp = FileOpener(dp, mode='rt')  \n    dp = dp.parse_csv()  \n    return dp\n","test":"data_dir = '<real_path>\/.'#the path has 1 csv file and this csv file has 1000 lines\nassert len(list(enumerate(generate_ans(data_dir))) == <ground_truth>)","result":"1000","code_lines":8,"cyclomatic_complexity":2}
{"ID":"113","code":"from torchdata.datapipes.iter import *\ndef separator(data):\n    return data.split(' ')\ndef generate_ans(file_path, separator=separator):\n    datapipe = (\n                FileOpener(file_path, mode='rt')\n                .readlines(return_path=False)\n                .map(separator)\n            )\n    return datapipe\n","test":"file_path = ['<real_path>\/test_data.txt']#this txt file has 20 lines\nassert len(list(generate_ans(file_path,separator))) == <ground_truth>","result":"20","code_lines":9,"cyclomatic_complexity":2}
{"ID":"116","code":"from torchdata.datapipes.iter import FileLister, FileOpener\ndef decode(item):\n    key, value = item\n    if key.endswith(\".txt\"):\n        return key, value.read().decode(\"utf-8\")\n    if key.endswith(\".bin\"):\n        return key, value.read().decode(\"utf-8\")\n    \ndef generate_ans(file_dir):\n    datapipe1 = FileLister(file_dir, \"*.tar\")\n    datapipe2 = FileOpener(datapipe1, mode=\"b\")\n    dataset = datapipe2.load_from_tar().map(decode).webdataset()\n    return dataset\n","test":"file_dir = '<real_path>\/'#this path has 1 tar file and this tar file has 1 .bin file and 1 .txt file\nassert len(list(generate_ans(file_dir))) == <ground_truth>","result":"1","code_lines":11,"cyclomatic_complexity":4}
{"ID":"12","code":"from torchdata.datapipes.iter import *\ndef generate_ans(source_datapipe, batch_size=64):\n    data = source_datapipe.shuffle().batch(batch_size=batch_size).collate()\n    return data\n","test":"source_datapipe = IterableWrapper(list(range(9)))\nbatch_size = 3\nassert len(list(generate_ans(source_datapipe,batch_size))) == <ground_truth>","result":"3","code_lines":3,"cyclomatic_complexity":1}
{"ID":"120","code":"from torchdata.datapipes.iter import *\ndef generate_ans(csv_file_list, len=1000, batch_size=2):\n    new_dp = FileOpener(csv_file_list)\n    new_dp = new_dp.parse_csv(skip_lines=1)\n    new_dp = new_dp.shuffle(buffer_size=len)\n    new_dp = new_dp.sharding_filter()\n    new_dp = new_dp.batch(batch_size=batch_size, drop_last=True)\n    return new_dp\n","test":"csv_file_list = ['<real_path>\/test_data.csv']#this csv file has 1000 lines\nlen = 1000\nbatch_size = 2\nassert len(list(generate_ans(csv_file_list, len, batch_size))) == <ground_truth>","result":"499","code_lines":7,"cyclomatic_complexity":1}
{"ID":"124","code":"from torchdata.datapipes.map import SequenceWrapper, Mapper\ndef increment_fn(data):\n    return data + 1\ndef generate_ans(data_list,increment_fn=increment_fn):\n    dp = SequenceWrapper(data_list)\n    map_dp_1 = dp.map(increment_fn)  \n    batch_dp = map_dp_1.batch(batch_size=2)\n    return batch_dp\n","test":"data_list = range(10)\nassert len(list(generate_ans(data_list, increment_fn))) == <ground_truth>","result":"5","code_lines":7,"cyclomatic_complexity":2}
{"ID":"13","code":"from torchdata.datapipes.iter import *  \nimport torch  \ndef filter_fn(fname: str) -> bool:\n    return fname.endswith(\"csv\")\n  \ndef generate_ans(  \n        root: str,  \n        batch_size: int) -> IterDataPipe[torch.Tensor]:  \n    dp = FileLister(root=root, recursive=True).filter(filter_fn)  \n    dp = FileOpener(dp, mode=\"rb\")  \n  \n    dp = dp.parse_csv(return_path=False)  \n    dp = dp.map(lambda x: torch.as_tensor([float(o) for o in x]))  \n  \n    dp = dp.shuffle()  \n  \n    dp = dp.batch(batch_size=batch_size, drop_last=False)  \n    return dp.collate(torch.stack)  \n","test":"root = '<real_path>\/.'# has 1 csv file and this csv file has 1000 lines\nbatch_size = 16\nassert len(list(generate_ans(root,batch_size))) == <ground_truth>","result":"63","code_lines":13,"cyclomatic_complexity":2}
{"ID":"14","code":"from torchdata.datapipes.iter import *\ndef generate_ans(data_list,worker_id_list):\n    data_stream = IterableWrapper(data_list)\n    worker_id_stream = IterableWrapper(worker_id_list)\n    data = Zipper(data_stream,worker_id_stream)\n    return data\n","test":"data_list = list(range(9))\nworker_id_list = list(range(9))\nassert len(list(generate_ans(data_list,worker_id_list))) == <ground_truth>","result":"9","code_lines":5,"cyclomatic_complexity":1}
{"ID":"15","code":"import sys\nfrom torch.utils.data.datapipes.iter import *\nfrom torchdata.dataloader2 import *\ndef generate_ans(data_list):\n    pipe1 = IterableWrapper(data_list).shuffle().sharding_filter()\n    return pipe1\n","test":"data_list = range(40)\nassert len(list(generate_ans(data_list))[0]) == <ground_truth>","result":"40","code_lines":4,"cyclomatic_complexity":1}
{"ID":"16","code":"from torchdata.datapipes.iter import *\ndef generate_ans(file_list):\n    dp = IterableWrapper(file_list)\n    dp = FileOpener(dp, mode='b')\n    dp = dp.parse_json_files()\n    return dp.shuffle().sharding_filter()\n","test":"file_list = ['<real_path>\/test_data.json']\nassert len(list(generate_ans(file_list))) == <ground_truth>","result":"1","code_lines":5,"cyclomatic_complexity":1}
{"ID":"17","code":"from torchdata.datapipes.iter import *\nfrom torchdata.datapipes.utils import *\ndef generate_ans(file_list):\n    dp = IterableWrapper(file_list)\n    dp = FileOpener(dp, mode='b')\n    dp = Decompressor(dp)\n    dp = StreamWrapper(dp)\n    return dp\n","test":"file_list = ['<real_path>\/test_data.tar']#this tar file can be depressed to 1 .bin file and 1 .txt file\nassert len(list(generate_ans(file_list))) == <ground_truth>","result":"1","code_lines":6,"cyclomatic_complexity":1}
{"ID":"18","code":"from torchdata.datapipes.iter import *\ndef generate_ans(data_list,worker_id_list):\n    worker_id_stream = IterableWrapper(worker_id_list)\n    zipped_stream = IterableWrapper(data_list)\n    zipped_stream = zipped_stream.zip(worker_id_stream)\n    return zipped_stream\n","test":"data_list = list(range(9))\nworker_id_list = list(range(9))\nassert len(list(generate_ans(data_list, worker_id_list))) == <ground_truth>","result":"9","code_lines":5,"cyclomatic_complexity":1}
{"ID":"19","code":"import os\nfrom torchdata.datapipes.iter import IoPathFileOpener, IterableWrapper\ndef generate_ans(tar_list):\n    urls = IterableWrapper(tar_list).shuffle()\n    dataset = IoPathFileOpener(urls, mode=\"rb\").load_from_tar()   \n    samples = dataset.groupby(lambda x:     \n                               os.path.basename(x[0]).split(\".\")[0],\n                           group_size=2, guaranteed_group_size=2)\n    dataset = samples.map(lambda x: \n                  {'a': x[0][1].read(),\n                   'b': x[0][1].read()})\n    dataset = dataset.shuffle()\n    return dataset\n","test":"tar_list = ['<real_path>\/test_data.tar']\nassert len(list(generate_ans(tar_list))) == <ground_truth>","result":"1","code_lines":12,"cyclomatic_complexity":1}
{"ID":"2","code":"from torchdata.datapipes.iter import *\ndef generate_ans(num_list):\n    dp = IterableWrapper(num_list).shuffle().sharding_filter()\n    return dp\n","test":"num_list = list([1, 2, 3, 4, 5])\nassert len(list(generate_ans(num_list)))","result":"5","code_lines":3,"cyclomatic_complexity":1}
{"ID":"23","code":"from torch import tensor\nfrom torchdata.dataloader2 import DataLoader2\nfrom torchdata.datapipes.iter import *\ndef collate_fn( batch):\n    new_batch = tensor(batch)\n    return new_batch\ndef generate_ans(datapipe, batch_size, drop_last):\n    labelpipe, datapipe = datapipe.unzip(sequence_length=2)\n    datapipe = datapipe.batch(batch_size=batch_size, drop_last=drop_last)\n    labelpipe = labelpipe.batch(batch_size=batch_size, drop_last=drop_last)\n    datapipe = datapipe.collate(collate_fn)\n    labelpipe = labelpipe.collate(collate_fn)\n    datapipe = datapipe.zip(labelpipe)\n    return datapipe\n","test":"datapipe = IterableWrapper(range(40)).zip(IterableWrapper(range(40)))\nbatch_size = 2\ndrop_last = True\nassert len(list(generate_ans(datapipe, batch_size, drop_last))) == <ground_truth>","result":"20","code_lines":11,"cyclomatic_complexity":2}
{"ID":"25","code":"from torchdata.datapipes.iter import FileLister, IterableWrapper\nfrom torch.utils.data.datapipes.utils.decoder import imagehandler\nimport time\ndef generate_ans(file_dir, buffer_size):\n    dp = (FileLister(file_dir,\"*.tar\")\n    .open_files(mode=\"b\")\n    .load_from_tar()\n    .routed_decode(imagehandler(\"torch\"))\n    .shuffle(buffer_size=buffer_size))\n    \n    return dp\n","test":"file_dir = '<real_path>\/test_data.tar'#this tar file has 1 .bin file and 1 .txt file\nbuffer_size = 100\nassert len(list(generate_ans(file_dir, buffer_size))) == <ground_truth>","result":"2","code_lines":8,"cyclomatic_complexity":1}
{"ID":"27","code":"from torchdata.datapipes.iter import *\ndef row_processer(row):\n    return {\"content\": row[0], \"label\": row[1]}\ndef generate_ans(path):\n    datapipe = FileLister(path)\n    datapipe = FileOpener(datapipe, encoding='utf-8')\n    datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=1)\n    datapipe = datapipe.shuffle()\n    datapipe = datapipe.map(row_processer)\n    return datapipe\n","test":"path = '<real_path>\/test_data.csv'# this csv file has 1000 lines\nassert len(list(generate_ans(path))) ==  <ground_truth>","result":"999","code_lines":9,"cyclomatic_complexity":2}
{"ID":"3","code":"from torchdata.datapipes.iter import *\ndef generate_ans(source_datapipe, batch_size=64):\n    data = source_datapipe.batch(batch_size=batch_size, drop_last=False).shuffle()\n    return data\n","test":"source_datapipe = IterableWrapper(list(range(9)))\nbatch_size = 3\nassert len(list(generate_ans(source_datapipe, batch_size))) == <ground_truth>","result":"3","code_lines":3,"cyclomatic_complexity":1}
{"ID":"34","code":"from torchdata.datapipes.iter import *\nfrom typing import Dict, Any\ndef generate_ans(file_list: str, dataset_size: int) -> IterDataPipe[Dict[str, Any]]:\n    dp = IoPathFileLister(root =file_list)\n    dp = dp.shuffle()\n    dp = IoPathFileOpener(dp, mode=\"rb\")\n    return dp\n","test":"file_list = '<real_path>\/test_data.txt'\ndataset_size = 2\nassert len(list(generate_ans(file_list, dataset_size))) == <ground_truth>","result":"1","code_lines":5,"cyclomatic_complexity":1}
{"ID":"38","code":"from torchdata.datapipes.iter import *\ndef generate_ans(file_list):\n    label_dp = FileLister(file_list)\n    label_dp = FileOpener(label_dp, mode=\"b\")\n    label_dp = Decompressor(label_dp)\n    return label_dp\n","test":"file_list = ['<real_path>\/test_data.tar']#this tar file has 1 .bin file and 1 .txt file\nassert len(list(generate_ans(file_list))) == <ground_truth>","result":"1","code_lines":5,"cyclomatic_complexity":1}
{"ID":"39","code":"from torchdata.datapipes.map import SequenceWrapper, Mapper\ndef generate_ans(data_list):\n    dp = SequenceWrapper(data_list)\n    map_dp_1 = dp.map(lambda x: x + 1)  \n    batch_dp = map_dp_1.batch(batch_size=2)\n    return batch_dp\n","test":"data_list = range(10)\nassert len(list(generate_ans(data_list))) == <ground_truth>","result":"5","code_lines":5,"cyclomatic_complexity":1}
{"ID":"4","code":"from torchdata.datapipes.iter import *\nfrom torchdata.datapipes.map import *\nimport numpy as np\nimport numpy.typing as npt\nimport torch\nfrom PIL import Image\nfrom torchvision.models import alexnet, AlexNet_Weights\nfrom typing import List, Hashable, Callable, Sequence, Tuple\nfrom pathlib import Path\nweights = AlexNet_Weights.IMAGENET1K_V1\ndef collate_fn(\n    batch: Sequence[Tuple[torch.Tensor, str]]\n) -> Tuple[torch.Tensor, npt.NDArray[np.str_]]:\n    images = torch.stack([pair[0] for pair in batch])\n    ids = np.array([pair[1] for pair in batch])\n    return images, ids\ndef preprocess(image: Image) -> torch.Tensor:\n    return weights.transforms()(image)\ndatapipe = SequenceWrapper(list(Path(\"<real_path>\/test_images\").rglob(\"*.jpg\")))\ndef generate_ans(\n    datapipe: MapDataPipe,\n    *,\n    preprocess_fn: Callable[[Image.Image], torch.Tensor],\n    batch_size: int,\n    indices: List[Hashable] = None,\n) -> IterDataPipe:\n    return         datapipe.to_iter_datapipe(indices=indices).map(fn=preprocess_fn).zip(IterableWrapper(indices)).batch(batch_size=batch_size).collate(collate_fn=collate_fn)\n","test":"#the file folder <test_images> has 3 jpg files\ndatapipe = datapipe\nassert len(generate_ans(datapipe, indices=[stimulus.name for stimulus in datapipe], preprocess_fn=preprocess, batch_size=64)) == <ground_truth>","result":"1","code_lines":21,"cyclomatic_complexity":3}
{"ID":"41","code":"from torchdata.datapipes.iter import *\ndef generate_ans(file_path: str, batch_size: int = 5):\n    buffer_size = 10000\n    dp = FileOpener([file_path], encoding='utf-8').parse_csv(skip_lines=1).shuffle(buffer_size=buffer_size)\n    dp = dp.sharding_filter().batch(batch_size=batch_size, drop_last=True)\n    return dp\n","test":"file_path = '<real_path>\/test_data.csv'#this csv file has 1000 lines\nbatch_size = 5\nassert len(list(generate_ans(file_path, batch_size))) == <ground_truth>","result":"199","code_lines":5,"cyclomatic_complexity":1}
{"ID":"42","code":"from torchdata.datapipes.iter import *\ndef generate_ans(url_list):\n    dp = IterableWrapper(url_list)\n    dp = HttpReader(dp)\n    return dp.readlines(return_path=False)\n","test":"url_list = ['http:\/\/www.idris.fr\/media\/formations\/fortran\/idris_fortran_base_exemples.tar.gz']#this file totally has 825 lines\nassert list(generate_ans(url_list)) == <ground_truth>","result":"825","code_lines":4,"cyclomatic_complexity":1}
{"ID":"45","code":"from torchdata.datapipes.iter import *\ndef filter_fn(file_path):\n    return file_path.endswith(\".jpg\")\ndef generate_ans(file_list):\n    datapipe = FileLister(file_list).filter(filter_fn)\n    datapipe = datapipe.shuffle() \n    datapipe = datapipe.sharding_filter()\n    return datapipe\n","test":"file_list = ['<real_path>\/test_images\/']#this file folder has 3 jpg files\nassert len(list(generate_ans(file_list))) == <ground_truth>","result":"3","code_lines":7,"cyclomatic_complexity":2}
{"ID":"46","code":"from torchdata.datapipes.iter import *\ndef generate_ans(url_list):\n    url_dp = IterableWrapper(url_list)\n    cache_dp = HttpReader(url_dp)\n    return cache_dp.parse_csv()\n","test":"url_list = ['https:\/\/raw.githubusercontent.com\/mhjabreel\/CharCnn_Keras\/master\/data\/ag_news_csv\/test.csv']#this csv file has 7600 lines\nassert len(list(generate_ans(url_list))) == <ground_truth>","result":"7600","code_lines":4,"cyclomatic_complexity":1}
{"ID":"47","code":"import os\nfrom typing import Union, Tuple\nfrom torchtext._internal.module_utils import is_module_available\nfrom torchtext.data.datasets_utils import (\n    _wrap_split_argument,\n    _create_dataset_directory,\n)\nif is_module_available(\"torchdata\"):\n    from torchdata.datapipes.iter import FileOpener, HttpReader, IterableWrapper\nURL = {\n    \"train\": \"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/train-v1.1.json\",\n    \"dev\": \"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/dataset\/dev-v1.1.json\",\n}\nMD5 = {\n    \"train\": \"981b29407e0affa3b1b156f72073b945\",\n    \"dev\": \"3e85deb501d4e538b6bc56f786231552\",\n}\nNUM_LINES = {\n    \"train\": 87599,\n    \"dev\": 10570,\n}\nDATASET_NAME = \"SQuAD1\"\ndef get_name(path_and_stream):\n    return os.path.basename(path_and_stream[0]), path_and_stream[1]\ndef generate_ans(cache_dir: str, split: Union[Tuple[str], str]):\n    # Args:\n    #     cache_dir: Directory where the datasets are saved. Default: os.path.expanduser('~\/.torchtext\/cache')\n    #     split: split or splits to be returned. Can be a string or tuple of strings. Default: (`train`, `dev`)\n    # :returns: DataPipe that yields data points from SQuaAD1 dataset which consist of context, question, list of answers and corresponding index in context\n    # :rtype: (str, str, list(str), list(int))\n    if not is_module_available(\"torchdata\"):\n        raise ModuleNotFoundError(\n            \"Package `torchdata` not found. Please install following instructions at `https:\/\/github.com\/pytorch\/data`\"\n        )\n    url_dp = IterableWrapper([URL[split]])\n    cache_dp = url_dp.on_disk_cache(\n        filepath_fn=lambda x: os.path.join(cache_dir, os.path.basename(x)),\n        hash_dict={os.path.join(cache_dir, os.path.basename(URL[split])): MD5[split]},\n        hash_type=\"md5\",\n    )\n    cache_dp = HttpReader(cache_dp).end_caching(mode=\"wb\", same_filepath_fn=True)\n    cache_dp = FileOpener(cache_dp, encoding=\"utf-8\")\n    return cache_dp.parse_json_files().read_squad()\n","test":"#In the dev set, there are 10,570 (context, question, answers, answer_start) sample pairs.\ncache_dir ='<real_path>\/tmp_files'\nsplit = 'dev'\nassert len(list(generate_ans(cache_dir, split))) == <ground_truth>","result":"10570","code_lines":39,"cyclomatic_complexity":5}
{"ID":"48","code":"from torchdata.datapipes.iter import FileOpener, HttpReader, IterableWrapper\ndef generate_ans(url_list):\n    url_dp = IterableWrapper(url_list)\n    cache_dp = HttpReader(url_dp)\n    return cache_dp.readlines(return_path=False).map(lambda t: t.strip())\n","test":"url_list = ['https:\/\/raw.githubusercontent.com\/wojzaremba\/lstm\/master\/data\/ptb.test.txt']#this txt file has 3761 lines\nassert len(list(generate_ans(url_list))) == <ground_truth>","result":"3761","code_lines":4,"cyclomatic_complexity":1}
{"ID":"5","code":"from torchdata import datapipes\nfrom torchdata.datapipes.iter import *\nfrom argparse import Namespace\nfrom functools import partial\nargs = Namespace(\n    # data\n    ## Flag\n    path_check = True,\n    is_content = False,\n    url = \"https:\/\/www.gutenberg.org\/files\/84\/84-0.txt\",\n    ## path and file name\n    data_base_path = \"<real_path>\/outputs\/\",\n    filename = \"frankenstein.txt\",\n    start_tkn = \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n    end_tkn = \"*** END OF THE PROJECT GUTENBERG EBOOK \",\n)\ndef para_joiner_fn(line):\n    return \" \".join(line)\ndef drop_filename_fn(tuples):\n    return tuples[1]\ndef filter_content(args,para):\n    if args.start_tkn in para:\n        args.is_content = True\n    elif args.is_content and args.end_tkn in para:\n        args.is_content = False\n    return args.is_content\ndef generate_ans(http_list):\n    dp = HttpReader(http_list).readlines(encoding=\"utf-8-sig\",decode=True).lines_to_paragraphs(para_joiner_fn).map(drop_filename_fn).filter(partial(filter_content,args))\n    return dp\n","test":"http_list = ['https:\/\/www.gutenberg.org\/files\/84\/84-0.txt']#this txt has 796 paragraphs\nassert list(generate_ans(http_list)) == <ground_truth>","result":"796","code_lines":25,"cyclomatic_complexity":6}
{"ID":"53","code":"\nfrom torchdata.datapipes.iter import HttpReader, IterableWrapper\ndef generate_ans(url_list):\n    out_pr = HttpReader(\n        IterableWrapper(url_list))\n    return out_pr.readlines(return_path=False)\n","test":"url_list = ['https:\/\/raw.githubusercontent.com\/wojzaremba\/lstm\/master\/data\/ptb.test.txt']#this txt file has 3761 lines\nassert list(generate_ans(url_list)) == <ground_truth>","result":"3761","code_lines":4,"cyclomatic_complexity":1}
{"ID":"54","code":"import torch\nfrom torchdata.datapipes.iter import *\ndef collate_fn(batch):\n    xs = [sample[0] + sample[1] for sample in batch]\n    ys = [sample[2] for sample in batch] \n    return torch.tensor(xs), torch.tensor(ys)\ndef generate_ans(data_set, batch_sz, collate_fn=collate_fn):\n    data_set = data_set.batch(batch_sz).collate(collate_fn)\n    data_set = data_set.shuffle()\n    return data_set\n","test":"data_set = IterableWrapper([(0, 0, 0),(1, 1, 1),(2, 2, 2),(3, 3, 3)])\nbatch_sz = 2\nassert len(list(generate_ans((Zipper(IterableWrapper(range(10)), IterableWrapper(range(10)), IterableWrapper(range(10)))), batch_sz=2))) == <ground_truth>","result":"5","code_lines":9,"cyclomatic_complexity":2}
{"ID":"56","code":"from torchdata.datapipes.iter import *\nfrom torchdata.datapipes.map import *\ndef generate_ans(map_dp):\n    dp = IterableWrapper(map_dp)\n    batch_dp = dp.enumerate().shuffle()\n    return batch_dp\n","test":"map_dp = SequenceWrapper(range(10))\n assert len(list(generate_ans(map_dp))) == <ground_truth>","result":"10","code_lines":4,"cyclomatic_complexity":1}
{"ID":"57","code":"from torchdata.datapipes.iter import *\ndef generate_ans(url_list) -> IterDataPipe:\n    datapipe = HttpReader(IterableWrapper(url_list))\n    datapipe = datapipe.parse_csv_as_dict()\n    datapipe = datapipe.in_memory_cache()\n    return datapipe\n","test":"url_list = ['https:\/\/raw.githubusercontent.com\/mhjabreel\/CharCnn_Keras\/master\/data\/ag_news_csv\/test.csv']#this csv file has 7599 lines\nassert len(list(generate_ans(url_list))) == <ground_truth>","result":"7599","code_lines":5,"cyclomatic_complexity":1}
{"ID":"58","code":"from torchdata.datapipes.iter import *\ndef filter_fn(path: str) -> bool:\n    return path.endswith('.txt')\ndef generate_ans(file_dir, filter_fn) -> IterDataPipe:\n    datapipe = FileLister([file_dir], masks='*.txt', recursive=True)\n    datapipe = datapipe.filter(filter_fn)\n    datapipe = datapipe.in_memory_cache() \n    return datapipe\n","test":"file_dir = '<real_path>\/'#this file folder has totally 2 txt files\nassert len(list(generate_ans(file_dir, filter_fn))) == <ground_truth>","result":"2","code_lines":7,"cyclomatic_complexity":2}
{"ID":"6","code":"from torchdata.datapipes.iter import *\nfrom functools import partial\nimport functools\nimport torch\ndef tokenize_space(data, symbol_table, UNK='<unk>'):\n    filename, text = data\n    tokens = text.split(\" \")\n    ids = [\n        symbol_table[token] if token in symbol_table else symbol_table[UNK]\n        for token in tokens\n    ]\n    return (filename, text, tokens, ids)\ndef read_symbol_table(symbol_table_file):\n    symbol_table = {}\n    with open(symbol_table_file, 'r', encoding='utf8') as fin:\n        for line in fin:\n            arr = line.strip().split()\n            symbol_table[arr[0]] = int(arr[1])\n    return symbol_table\ndef filter_fn(data):\n    if len(data) > 1:\n        return True\n    else:\n        return False\ndef padding_batch(data, padding_value=0):\n    f, t, tk, l = [], [], [], []\n    l = []\n    for sample in data:\n        filename, text, tokens, ids = sample\n        l.append(torch.tensor(ids, dtype=torch.int, requires_grad=False))\n        f.append(filename)\n        t.append(text)\n        tk.append(tokens)\n    labels_tensor = torch.nn.utils.rnn.pad_sequence(l, batch_first=True)\n    return f, t, tk, labels_tensor\nsymbol_table = read_symbol_table(\"<real_path>\/unit.txt\")\ntokenize_fn = functools.partial(tokenize_space, symbol_table=symbol_table)\ndef generate_ans(\n    data_list_file,\n    buffer_size,  \n    batch_size=1\n) -> IterDataPipe:\n    dataset = FileOpener(data_list_file)\n    dataset = dataset.readlines()\n    dataset = dataset.map(tokenize_fn)\n    dataset = dataset.filter(filter_fn)\n    dataset = dataset.shuffle(\n        buffer_size=buffer_size)\n    dataset = dataset.sharding_filter()\n    dataset = dataset.batch(\n        batch_size=batch_size\n    )\n    dataset = dataset.collate(padding_batch)\n    return dataset\n","test":"data_list_file = ['<real_path>\/test_data.txt']\nbuffer_size = 2\nbatch_size = 2\nassert len(list(generate_ans(data_list_file,buffer_size,batch_size))) == <ground_truth>#the final batches is 10","result":"10","code_lines":52,"cyclomatic_complexity":8}
{"ID":"60","code":"from torchdata.datapipes.iter import * \ndef generate_ans(dir_list, total_length):\n    datapipe = FileLister(dir_list).filter(filter_fn=lambda filename: filename.endswith('.csv'))\n    datapipe = FileOpener(datapipe, mode='rt')\n    datapipe = datapipe.parse_csv(delimiter=',')\n    train, valid = datapipe.random_split(total_length=total_length, weights={\"train\": 0.5, \"valid\": 0.5}, seed=0)\n    return train, valid\n","test":"dir_list = ['<real_path>\/']\ntotal_length = 16\nassert len(list(generate_ans(dir_list, total_length))) == <ground_truth>","result":"2","code_lines":6,"cyclomatic_complexity":1}
{"ID":"61","code":"from torchdata.datapipes.iter import *\nimport numpy as np\ndef filter_fn(filename):\n    return filename.endswith(\".csv\")\ndef row_processer(row):\n    return {\"label\": np.array(row[0], np.int32), \"data\": np.array(row[1:], dtype=np.float64)}\ndef generate_ans(root_dir=\".\", filter_fn=filter_fn, row_processer=row_processer):\n    datapipe = FileLister(root_dir)\n    datapipe = datapipe.filter(filter_fn=filter_fn)\n    datapipe = datapipe.open_files(mode='rt')\n    datapipe = datapipe.parse_csv(delimiter=\",\", skip_lines=1)\n    # Shuffle will happen as long as you do NOT set `shuffle=False` later in the DataLoader\n    datapipe = datapipe.shuffle()\n    datapipe = datapipe.map(row_processer)\n    return datapipe\n","test":"root_dir = '<real_path>\/'#the file folder only has 1 csv file and this csv file has 1000 lines\nassert len(list(generate_ans(root_dir, filter_fn, row_processer))) == <ground_truth>","result":"999","code_lines":14,"cyclomatic_complexity":3}
{"ID":"7","code":"from torchdata.datapipes.iter import *\nfrom torchdata.dataloader2 import DataLoader2\ndef generate_ans(data_list, batch_size):\n    pipe = IterableWrapper(data_list)\n    pipe = pipe.batch(batch_size).sharding_filter()\n    dl = DataLoader2(pipe)\n    return dl\n","test":"data_list = list(range(9 * 10))\nbatch_size = 10\nassert len(list(generate_ans(data_list, batch_size))) == <ground_truth>","result":"9","code_lines":5,"cyclomatic_complexity":1}
{"ID":"8","code":"import torch\nfrom torchdata.datapipes.iter import IterableWrapper\ndef decode(d):\n    torch.frombuffer(d[1], dtype=torch.uint8).clone()\n    return torch.zeros([3,10,10])\ndef generate_ans(files,batch_size):\n    dp = IterableWrapper(files).sharding_filter().open_files_by_fsspec(mode='rb')\n    dp = dp.read_from_stream().map(fn=decode).batch(batch_size).collate()\n    return dp\n","test":"files = ['<real_path>\/test_data.txt']\nbatch_size = 1\nassert list(generate_ans(files, batch_size))[0].shape == <ground_truth>","result":"(1, 3, 10, 10)","code_lines":8,"cyclomatic_complexity":2}
{"ID":"83","code":"\nfrom torchdata.datapipes.iter import IterableWrapper\ndef generate_ans(csv_fils_list):\n    dp = (IterableWrapper(csv_fils_list)\n            .open_files_by_fsspec() \n            .parse_csv())\n    return dp\n","test":"csv_fils_list = ['<real_path>\/test_data.csv']#this csv file has 1000 lines\nassert len(list(generate_ans(csv_fils_list))) == <ground_truth>","result":"1000","code_lines":5,"cyclomatic_complexity":1}
{"ID":"9","code":"from torchdata.datapipes.iter import IterableWrapper\ndef open_read(batch):\n    batch = [open(item, 'rb').read() for item in batch]\n    return batch\ndef generate_ans(files, batch_size):\n    dp = IterableWrapper(files).sharding_filter().batch(batch_size).map(fn=open_read)\n    \n    dp = dp.collate()\n    return dp\n","test":"files = ['<real_path>\/test_images\/wnid_1.jpg']\nbatch_size = 1\nassert len(list(generate_ans(files, batch_size))) == <ground_truth>","result":"1","code_lines":7,"cyclomatic_complexity":2}
{"ID":"92","code":"from typing import Callable, Optional, Collection\nfrom typing import Union, Tuple, List, Dict, Any\nimport pathlib\nimport torch\nfrom torchdata.datapipes.iter import *\nimport functools\nimport io\nimport os\ndef _is_not_top_level_file(path: str, *, root: pathlib.Path) -> bool:\n    rel_path = pathlib.Path(path).relative_to(root)\n    return rel_path.is_dir() or rel_path.parent != pathlib.Path(\".\")\ndef _collate_and_decode_data(\n    data: Tuple[str, io.IOBase],\n    *,\n    root: pathlib.Path,\n    categories: List[str],\n    decoder: Optional[Callable[[io.IOBase], torch.Tensor]],\n) -> Dict[str, Any]:\n    path, buffer = data\n    data = decoder(buffer) if decoder else buffer\n    category = pathlib.Path(path).relative_to(root).parts[0]\n    label = torch.tensor(categories.index(category))\n    return dict(\n        path=path,\n        data=data,\n        label=label,\n        category=category,\n    )\ndef generate_ans(\n    root: Union[str, pathlib.Path],\n    *,\n    decoder: Optional[Callable[[io.IOBase], torch.Tensor]] = None,\n    valid_extensions: Optional[Collection[str]] = None,\n    recursive: bool = True,\n) -> Tuple[IterDataPipe, List[str]]:\n    root = pathlib.Path(root).expanduser().resolve()\n    categories = sorted(entry.name for entry in os.scandir(root) if entry.is_dir())\n    masks: Union[List[str], str] = [f\"*.{ext}\" for ext in valid_extensions] if valid_extensions is not None else \"\"\n    dp = FileLister(str(root), recursive=recursive, masks=masks)\n    dp: IterDataPipe = Filter(dp, functools.partial(_is_not_top_level_file, root=root))\n    # dp = hint_sharding(dp)\n    dp = Shuffler(dp)\n    dp = FileOpener(dp, mode=\"rb\")\n    return (\n        Mapper(dp, functools.partial(_collate_and_decode_data, root=root, categories=categories, decoder=decoder)),\n        categories,\n    )\n","test":"root = '<real_path>\/'\nassert len(list(generate_ans(root))) == <ground_truth>","result":"2","code_lines":44,"cyclomatic_complexity":3}
{"ID":"94","code":"from torchdata.datapipes.iter import *\ndef generate_ans(data_list, batch_size):\n    dp = (IterableWrapper(data_list) \n            .shuffle() \n            .enumerate() \n            .batch(batch_size=batch_size))\n    return dp\n","test":"data_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nbatch_size = 3\nassert len(list(generate_ans(data_list, batch_size))) == <ground_truth>","result":"4","code_lines":6,"cyclomatic_complexity":1}
{"ID":"98","code":"from torchdata.datapipes.iter import *\ndef generate_ans(file_dir = \".\"):\n    dp = FileLister(file_dir, \"*.zip\")\n    dp = FileOpener(dp, mode=\"b\")\n    dp = dp.load_from_zip()\n    return dp\n","test":"file_dir = '<real_path>\/'#this file folder only has 1 zip file and this zip file only has 1 file\nassert len(list(generate_ans(file_dir))) == <ground_truth>","result":"1","code_lines":5,"cyclomatic_complexity":1}
